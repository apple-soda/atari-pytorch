{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d986cb2-e0b7-4746-8d93-608a3e291c21",
   "metadata": {},
   "source": [
    "# Same notebook as breakout.ipynb, but using OpenAI's atari wrappers instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "140a85c8-4925-4b19-a100-d79c12c2d726",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "59f51ede-7f0a-48da-a161-5948950d177a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ddqn import *\n",
    "from environments.openai import *\n",
    "from utils.train import *\n",
    "from utils.logger import *\n",
    "from utils.render import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9afcc9bf-dacb-40fb-bad4-414beadb0c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fef795f5-0226-493c-8dc9-f0de939d8dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize environment\n",
    "raw_env = gym.make('BreakoutNoFrameskip-v4')\n",
    "env = Wrap_Deepmind(raw_env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78abe324-6f3c-452f-876b-ba4913c060d7",
   "metadata": {},
   "source": [
    "### Scaling hyperparameters in accordance with environment parameter changes\n",
    "#### num_steps += 1 represents 4 frames (so really num_steps += 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a32f328-792e-43ea-bf4b-fb4351c5692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize agent\n",
    "observation_space = raw_env.observation_space\n",
    "action_space = raw_env.action_space\n",
    "\n",
    "params = {'epsilon':1.0, 'epsilon_min':0.1, 'epsilon_decay': None, 'eps_ff': 1000000, 'eps_interval':0.9, 'eps_start':1.0, 'gamma':0.99, 'alpha':2.5e-5, \n",
    "          'network_params': None, 'memory_size':150000, 'device':'cuda:0', 'batch_size':32, 'target_net_updates':2500}\n",
    "\n",
    "agent = DQNAgent(observation_space, action_space, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53e33fae-b95e-4720-8f92-d7552d7a3e36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepmindCNN(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db2cdd4b-90d3-435f-a136-22261d874dad",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('training_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da64f3f-cabb-42b4-9c8d-6845f9b7cd82",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../models/breakout/'\n",
    "training_params = {'total_steps':12000000, 'logger':logger, 'save_freq':100000, 'e_verbose':500000, 'file_name': 'breakout ddqn 2', 'save_dir':save_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caac7031-8df2-452c-8094-1c0596705be7",
   "metadata": {},
   "source": [
    "### Deepmind Atari Preprocesing:\n",
    "* Extremely high RAM usage\n",
    "* Frameskips between stacked inputs\n",
    "    * Each input spans 16 frames\n",
    "* Inputs have overlap\n",
    "    * (x1, x2, x3, x4) --env.step--> (x2, x3, x4, x5)\n",
    "    * x1 and x2 differ by 4 frames\n",
    "* READ: https://danieltakeshi.github.io/2016/11/25/frame-skipping-and-preprocessing-for-deep-q-networks-on-atari-2600-games/\n",
    "\n",
    "### My Atari Preprocessing:\n",
    "* Lower RAM usage\n",
    "* No frameskips between stacked inputs. \n",
    "    * Each input spans 4 frames\n",
    "* No overlap between frames unless environment termination\n",
    "    * (x1, x2, x3, x4) --env.step--> (x5, x6, x7, x8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1fd369d2-819e-4e81-81be-493185d2753a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 544: Saved model weights and log.\n",
      "Episode 1046: Saved model weights and log.\n",
      "Episode 1489: Saved model weights and log.\n",
      "Episode 1873: Saved model weights and log.\n",
      "Steps : 500000, Average Reward: 2.5944970681100585, Memory Length: 150000, Optimizer Steps: 500000, Time Elapsed: 4347.873561859131, Target Q Updates: 200\n",
      "Episode 2217: Saved model weights and log.\n",
      "Episode 2533: Saved model weights and log.\n",
      "Episode 2818: Saved model weights and log.\n",
      "Episode 3072: Saved model weights and log.\n",
      "Episode 3317: Saved model weights and log.\n",
      "Steps : 1000000, Average Reward: 8.869532428355958, Memory Length: 150000, Optimizer Steps: 1000000, Time Elapsed: 3928.729994058609, Target Q Updates: 400\n",
      "Episode 3543: Saved model weights and log.\n",
      "Episode 3783: Saved model weights and log.\n",
      "Episode 4038: Saved model weights and log.\n",
      "Episode 4292: Saved model weights and log.\n",
      "Episode 4531: Saved model weights and log.\n",
      "Steps : 1500000, Average Reward: 17.975550122249388, Memory Length: 150000, Optimizer Steps: 1500000, Time Elapsed: 3691.412124633789, Target Q Updates: 600\n",
      "Episode 4770: Saved model weights and log.\n",
      "Episode 5001: Saved model weights and log.\n",
      "Episode 5238: Saved model weights and log.\n",
      "Episode 5472: Saved model weights and log.\n",
      "Episode 5704: Saved model weights and log.\n",
      "Steps : 2000000, Average Reward: 25.239484978540773, Memory Length: 150000, Optimizer Steps: 2000000, Time Elapsed: 3666.7968220710754, Target Q Updates: 800\n",
      "Episode 5935: Saved model weights and log.\n",
      "Episode 6157: Saved model weights and log.\n",
      "Episode 6383: Saved model weights and log.\n",
      "Episode 6606: Saved model weights and log.\n",
      "Episode 6835: Saved model weights and log.\n",
      "Steps : 2500000, Average Reward: 30.599821746880572, Memory Length: 150000, Optimizer Steps: 2500000, Time Elapsed: 3665.8626594543457, Target Q Updates: 1000\n",
      "Episode 7057: Saved model weights and log.\n",
      "Episode 7278: Saved model weights and log.\n",
      "Episode 7498: Saved model weights and log.\n",
      "Episode 7722: Saved model weights and log.\n",
      "Episode 7949: Saved model weights and log.\n",
      "Steps : 3000000, Average Reward: 31.610762331838565, Memory Length: 150000, Optimizer Steps: 3000000, Time Elapsed: 3663.6834683418274, Target Q Updates: 1200\n",
      "Episode 8172: Saved model weights and log.\n",
      "Episode 8395: Saved model weights and log.\n",
      "Episode 8615: Saved model weights and log.\n",
      "Episode 8835: Saved model weights and log.\n",
      "Episode 9055: Saved model weights and log.\n",
      "Steps : 3500000, Average Reward: 32.21195652173913, Memory Length: 150000, Optimizer Steps: 3500000, Time Elapsed: 3666.699292898178, Target Q Updates: 1400\n",
      "Episode 9276: Saved model weights and log.\n",
      "Episode 9499: Saved model weights and log.\n",
      "Episode 9718: Saved model weights and log.\n",
      "Episode 9940: Saved model weights and log.\n",
      "Episode 10163: Saved model weights and log.\n",
      "Steps : 4000000, Average Reward: 31.528455284552845, Memory Length: 150000, Optimizer Steps: 4000000, Time Elapsed: 3651.900004863739, Target Q Updates: 1600\n",
      "Episode 10383: Saved model weights and log.\n",
      "Episode 10605: Saved model weights and log.\n",
      "Episode 10827: Saved model weights and log.\n",
      "Episode 11045: Saved model weights and log.\n",
      "Episode 11265: Saved model weights and log.\n",
      "Steps : 4500000, Average Reward: 32.41818181818182, Memory Length: 150000, Optimizer Steps: 4500000, Time Elapsed: 3651.1006786823273, Target Q Updates: 1800\n",
      "Episode 11483: Saved model weights and log.\n",
      "Episode 11713: Saved model weights and log.\n",
      "Episode 11933: Saved model weights and log.\n",
      "Episode 12156: Saved model weights and log.\n",
      "Episode 12382: Saved model weights and log.\n",
      "Steps : 5000000, Average Reward: 31.30384271671135, Memory Length: 150000, Optimizer Steps: 5000000, Time Elapsed: 3656.3621072769165, Target Q Updates: 2000\n",
      "Episode 12602: Saved model weights and log.\n",
      "Episode 12825: Saved model weights and log.\n",
      "Episode 13051: Saved model weights and log.\n",
      "Episode 13278: Saved model weights and log.\n",
      "Episode 13503: Saved model weights and log.\n",
      "Steps : 5500000, Average Reward: 32.1166518254675, Memory Length: 150000, Optimizer Steps: 5500000, Time Elapsed: 3660.165513277054, Target Q Updates: 2200\n",
      "Episode 13725: Saved model weights and log.\n",
      "Episode 13949: Saved model weights and log.\n",
      "Episode 14170: Saved model weights and log.\n",
      "Episode 14402: Saved model weights and log.\n",
      "Episode 14629: Saved model weights and log.\n",
      "Steps : 6000000, Average Reward: 31.828318584070797, Memory Length: 150000, Optimizer Steps: 6000000, Time Elapsed: 3658.4215092658997, Target Q Updates: 2400\n",
      "Episode 14855: Saved model weights and log.\n",
      "Episode 15079: Saved model weights and log.\n",
      "Episode 15303: Saved model weights and log.\n",
      "Episode 15524: Saved model weights and log.\n",
      "Episode 15749: Saved model weights and log.\n",
      "Steps : 6500000, Average Reward: 32.90017825311943, Memory Length: 150000, Optimizer Steps: 6500000, Time Elapsed: 3673.843166589737, Target Q Updates: 2600\n",
      "Episode 15977: Saved model weights and log.\n",
      "Episode 16206: Saved model weights and log.\n",
      "Episode 16433: Saved model weights and log.\n",
      "Episode 16658: Saved model weights and log.\n",
      "Episode 16881: Saved model weights and log.\n",
      "Steps : 7000000, Average Reward: 32.323294951284325, Memory Length: 150000, Optimizer Steps: 7000000, Time Elapsed: 3679.7107124328613, Target Q Updates: 2800\n",
      "Episode 17106: Saved model weights and log.\n",
      "Episode 17330: Saved model weights and log.\n",
      "Episode 17553: Saved model weights and log.\n",
      "Episode 17782: Saved model weights and log.\n",
      "Episode 18002: Saved model weights and log.\n",
      "Steps : 7500000, Average Reward: 33.1524064171123, Memory Length: 150000, Optimizer Steps: 7500000, Time Elapsed: 3684.373698949814, Target Q Updates: 3000\n",
      "Episode 18228: Saved model weights and log.\n",
      "Episode 18448: Saved model weights and log.\n",
      "Episode 18681: Saved model weights and log.\n",
      "Episode 18905: Saved model weights and log.\n",
      "Episode 19125: Saved model weights and log.\n",
      "Steps : 8000000, Average Reward: 34.456171735241504, Memory Length: 150000, Optimizer Steps: 8000000, Time Elapsed: 3667.479751110077, Target Q Updates: 3200\n",
      "Episode 19346: Saved model weights and log.\n",
      "Episode 19569: Saved model weights and log.\n",
      "Episode 19797: Saved model weights and log.\n",
      "Episode 20026: Saved model weights and log.\n",
      "Episode 20258: Saved model weights and log.\n",
      "Steps : 8500000, Average Reward: 31.683098591549296, Memory Length: 150000, Optimizer Steps: 8500000, Time Elapsed: 3712.8084604740143, Target Q Updates: 3400\n",
      "Episode 20482: Saved model weights and log.\n",
      "Episode 20707: Saved model weights and log.\n",
      "Episode 20936: Saved model weights and log.\n",
      "Episode 21163: Saved model weights and log.\n",
      "Episode 21391: Saved model weights and log.\n",
      "Steps : 9000000, Average Reward: 31.788038698328936, Memory Length: 150000, Optimizer Steps: 9000000, Time Elapsed: 3707.6979100704193, Target Q Updates: 3600\n",
      "Episode 21619: Saved model weights and log.\n",
      "Episode 21849: Saved model weights and log.\n",
      "Episode 22075: Saved model weights and log.\n",
      "Episode 22304: Saved model weights and log.\n",
      "Episode 22530: Saved model weights and log.\n",
      "Steps : 9500000, Average Reward: 32.94288224956063, Memory Length: 150000, Optimizer Steps: 9500000, Time Elapsed: 3703.387549161911, Target Q Updates: 3800\n",
      "Episode 22757: Saved model weights and log.\n",
      "Episode 22987: Saved model weights and log.\n",
      "Episode 23218: Saved model weights and log.\n",
      "Episode 23450: Saved model weights and log.\n",
      "Episode 23675: Saved model weights and log.\n",
      "Steps : 10000000, Average Reward: 33.161120840630474, Memory Length: 150000, Optimizer Steps: 10000000, Time Elapsed: 3695.730612754822, Target Q Updates: 4000\n",
      "Episode 23899: Saved model weights and log.\n",
      "Episode 24126: Saved model weights and log.\n",
      "Episode 24351: Saved model weights and log.\n",
      "Episode 24577: Saved model weights and log.\n",
      "Episode 24807: Saved model weights and log.\n",
      "Steps : 10500000, Average Reward: 33.71919014084507, Memory Length: 150000, Optimizer Steps: 10500000, Time Elapsed: 3701.161473751068, Target Q Updates: 4200\n",
      "Episode 25035: Saved model weights and log.\n",
      "Episode 25268: Saved model weights and log.\n",
      "Episode 25496: Saved model weights and log.\n",
      "Episode 25723: Saved model weights and log.\n",
      "Episode 25950: Saved model weights and log.\n",
      "Steps : 11000000, Average Reward: 33.40877192982456, Memory Length: 150000, Optimizer Steps: 11000000, Time Elapsed: 3695.486266374588, Target Q Updates: 4400\n",
      "Episode 26175: Saved model weights and log.\n",
      "Episode 26401: Saved model weights and log.\n",
      "Episode 26629: Saved model weights and log.\n",
      "Episode 26855: Saved model weights and log.\n",
      "Episode 27084: Saved model weights and log.\n",
      "Steps : 11500000, Average Reward: 35.60510114335972, Memory Length: 150000, Optimizer Steps: 11500000, Time Elapsed: 3695.6019864082336, Target Q Updates: 4600\n",
      "Episode 27312: Saved model weights and log.\n",
      "Episode 27534: Saved model weights and log.\n",
      "Episode 27759: Saved model weights and log.\n",
      "Episode 27981: Saved model weights and log.\n",
      "Episode 28204: Saved model weights and log.\n",
      "Steps : 12000000, Average Reward: 36.092046470062556, Memory Length: 150000, Optimizer Steps: 12000000, Time Elapsed: 3694.3082275390625, Target Q Updates: 4800\n",
      "Episode 28431: Saved model weights and log.\n"
     ]
    }
   ],
   "source": [
    "standard_train(agent, env, **training_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da19d3e6-4f61-43ba-97b5-f1d784019762",
   "metadata": {},
   "source": [
    "# if doesnt work, change noop, monitor, and firereset. all useless. also make it so the user can specify which environments to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e2cfcb04-e6f9-4988-ade3-01ea4359c966",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/breakout/breakout ddqn 2.pth'\n",
    "save = '../models/breakout/breakout ddqn 2 final.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1335cc24-d1b1-4d93-87ee-62262f7a317a",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51b4716a-55b5-4a33-a3cb-e9807f8e320c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_agent(agent, env, save, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bb4c0-0523-44e9-97cb-0bbabcc0d8e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
