{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfd16756-c60f-4c36-aeb0-7fcfe7debf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7774715-32f2-492f-8764-c480d60537c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.ddqn import *\n",
    "from utils.replay import *\n",
    "from environments.wrappers import *\n",
    "from networks.flexnet import *\n",
    "from utils.train import *\n",
    "from utils.logger import *\n",
    "from utils.render import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ef836ca-8e77-4e05-a4d6-f8d6f20415f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326018ac-6c77-4aa9-b8e3-fe43dc4997c0",
   "metadata": {},
   "source": [
    "### Network, Environment, Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d13fd47c-7819-49f5-b746-b180ff4facd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters\n",
    "img_size = (84, 84)\n",
    "num_stacked_frames = 4\n",
    "\n",
    "raw_env = gym.make('BreakoutNoFrameskip-v4')\n",
    "env = AtariWrapper(raw_env, k=num_stacked_frames, img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e41e57-b522-4d83-9d16-af89dfc488ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space = raw_env.observation_space\n",
    "action_space = raw_env.action_space\n",
    "\n",
    "params = {'epsilon':1.0, 'epsilon_min':0.1, 'epsilon_decay': None, 'eps_ff': 1000000, 'eps_interval':0.9, 'eps_start':1.0, 'gamma':0.99, 'alpha':2.5e-5, \n",
    "          'network_params': None, 'memory_size':150000, 'device':'cuda:0', 'batch_size':32, 'target_net_updates':10000}\n",
    "\n",
    "agent = DQNAgent(observation_space, action_space, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3550b6e7-8db5-4719-8449-3b7cc45b692e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepmindCNN(\n",
       "  (network): Sequential(\n",
       "    (0): Conv2d(4, 32, kernel_size=(8, 8), stride=(4, 4))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(32, 64, kernel_size=(4, 4), stride=(2, 2))\n",
       "    (3): ReLU()\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (5): ReLU()\n",
       "    (6): Flatten(start_dim=1, end_dim=-1)\n",
       "    (7): Linear(in_features=3136, out_features=512, bias=True)\n",
       "    (8): ReLU()\n",
       "    (9): Linear(in_features=512, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5a09e0-4d1f-4df9-9d96-64a481d90cbc",
   "metadata": {},
   "source": [
    "### Standard Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb838264-72f9-4f66-bae2-07b195443276",
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = Logger('training_info')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7d1e01d2-19d8-4501-88ce-b9be20cdfa18",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = '../models/breakout/'\n",
    "training_params = {'total_steps':20000000, 'logger':logger, 'save_freq':500000, 'e_verbose':50000, 'file_name': 'breakout ddqn', 'save_dir':save_dir}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "51f2c46f-e788-46e6-9ad5-f141e9ddf6ad",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Steps : 50000, Average Reward: 1.0210526315789474, Memory Length: 50000, Optimizer Steps: 50000, Time Elapsed: 285.00275683403015, Target Q Updates: 5\n",
      "Steps : 100000, Average Reward: 1.3037037037037038, Memory Length: 100000, Optimizer Steps: 100000, Time Elapsed: 357.76484966278076, Target Q Updates: 10\n",
      "Steps : 150000, Average Reward: 1.224264705882353, Memory Length: 150000, Optimizer Steps: 150000, Time Elapsed: 440.11166548728943, Target Q Updates: 15\n",
      "Steps : 200000, Average Reward: 1.6007905138339922, Memory Length: 150000, Optimizer Steps: 200000, Time Elapsed: 508.79707169532776, Target Q Updates: 20\n",
      "Steps : 250000, Average Reward: 2.2813852813852815, Memory Length: 150000, Optimizer Steps: 250000, Time Elapsed: 504.8848078250885, Target Q Updates: 25\n",
      "Steps : 300000, Average Reward: 2.5622119815668203, Memory Length: 150000, Optimizer Steps: 300000, Time Elapsed: 501.18783617019653, Target Q Updates: 30\n",
      "Steps : 350000, Average Reward: 2.517857142857143, Memory Length: 150000, Optimizer Steps: 350000, Time Elapsed: 476.5015754699707, Target Q Updates: 35\n",
      "Steps : 400000, Average Reward: 3.0693069306930694, Memory Length: 150000, Optimizer Steps: 400000, Time Elapsed: 473.76530170440674, Target Q Updates: 40\n",
      "Steps : 450000, Average Reward: 3.6296296296296298, Memory Length: 150000, Optimizer Steps: 450000, Time Elapsed: 459.56537532806396, Target Q Updates: 45\n",
      "Steps : 500000, Average Reward: 4.092485549132948, Memory Length: 150000, Optimizer Steps: 500000, Time Elapsed: 442.92422580718994, Target Q Updates: 50\n",
      "Episode 2316: Saved model weights and log.\n",
      "Steps : 550000, Average Reward: 4.104651162790698, Memory Length: 150000, Optimizer Steps: 550000, Time Elapsed: 435.08307433128357, Target Q Updates: 55\n",
      "Steps : 600000, Average Reward: 6.147887323943662, Memory Length: 150000, Optimizer Steps: 600000, Time Elapsed: 425.37725138664246, Target Q Updates: 60\n",
      "Steps : 650000, Average Reward: 8.08, Memory Length: 150000, Optimizer Steps: 650000, Time Elapsed: 412.8891279697418, Target Q Updates: 65\n",
      "Steps : 700000, Average Reward: 9.172413793103448, Memory Length: 150000, Optimizer Steps: 700000, Time Elapsed: 408.0795040130615, Target Q Updates: 70\n",
      "Steps : 750000, Average Reward: 10.552380952380952, Memory Length: 150000, Optimizer Steps: 750000, Time Elapsed: 403.70352029800415, Target Q Updates: 75\n",
      "Steps : 800000, Average Reward: 12.25, Memory Length: 150000, Optimizer Steps: 800000, Time Elapsed: 393.80050468444824, Target Q Updates: 80\n",
      "Steps : 850000, Average Reward: 14.411111111111111, Memory Length: 150000, Optimizer Steps: 850000, Time Elapsed: 386.6700131893158, Target Q Updates: 85\n",
      "Steps : 900000, Average Reward: 15.551724137931034, Memory Length: 150000, Optimizer Steps: 900000, Time Elapsed: 383.6032211780548, Target Q Updates: 90\n",
      "Steps : 950000, Average Reward: 16.04597701149425, Memory Length: 150000, Optimizer Steps: 950000, Time Elapsed: 377.2374258041382, Target Q Updates: 95\n",
      "Steps : 1000000, Average Reward: 19.19753086419753, Memory Length: 150000, Optimizer Steps: 1000000, Time Elapsed: 374.99938893318176, Target Q Updates: 100\n",
      "Episode 3421: Saved model weights and log.\n",
      "Steps : 1050000, Average Reward: 20.43243243243243, Memory Length: 150000, Optimizer Steps: 1050000, Time Elapsed: 373.59999084472656, Target Q Updates: 105\n",
      "Steps : 1100000, Average Reward: 21.08219178082192, Memory Length: 150000, Optimizer Steps: 1100000, Time Elapsed: 369.09901762008667, Target Q Updates: 110\n",
      "Steps : 1150000, Average Reward: 21.11111111111111, Memory Length: 150000, Optimizer Steps: 1150000, Time Elapsed: 368.8027470111847, Target Q Updates: 115\n",
      "Steps : 1200000, Average Reward: 20.591549295774648, Memory Length: 150000, Optimizer Steps: 1200000, Time Elapsed: 366.20838594436646, Target Q Updates: 120\n",
      "Steps : 1250000, Average Reward: 22.185714285714287, Memory Length: 150000, Optimizer Steps: 1250000, Time Elapsed: 363.95733618736267, Target Q Updates: 125\n",
      "Steps : 1300000, Average Reward: 20.541666666666668, Memory Length: 150000, Optimizer Steps: 1300000, Time Elapsed: 365.2194857597351, Target Q Updates: 130\n",
      "Steps : 1350000, Average Reward: 22.055555555555557, Memory Length: 150000, Optimizer Steps: 1350000, Time Elapsed: 363.77617144584656, Target Q Updates: 135\n",
      "Steps : 1400000, Average Reward: 21.14864864864865, Memory Length: 150000, Optimizer Steps: 1400000, Time Elapsed: 363.43185806274414, Target Q Updates: 140\n",
      "Steps : 1450000, Average Reward: 23.485294117647058, Memory Length: 150000, Optimizer Steps: 1450000, Time Elapsed: 363.7991921901703, Target Q Updates: 145\n",
      "Steps : 1500000, Average Reward: 21.763157894736842, Memory Length: 150000, Optimizer Steps: 1500000, Time Elapsed: 363.57976722717285, Target Q Updates: 150\n",
      "Episode 4143: Saved model weights and log.\n",
      "Steps : 1550000, Average Reward: 21.246753246753247, Memory Length: 150000, Optimizer Steps: 1550000, Time Elapsed: 363.55897855758667, Target Q Updates: 155\n",
      "Steps : 1600000, Average Reward: 19.404761904761905, Memory Length: 150000, Optimizer Steps: 1600000, Time Elapsed: 363.32838773727417, Target Q Updates: 160\n",
      "Steps : 1650000, Average Reward: 20.1, Memory Length: 150000, Optimizer Steps: 1650000, Time Elapsed: 364.71702790260315, Target Q Updates: 165\n",
      "Steps : 1700000, Average Reward: 19.609756097560975, Memory Length: 150000, Optimizer Steps: 1700000, Time Elapsed: 363.33010244369507, Target Q Updates: 170\n",
      "Steps : 1750000, Average Reward: 21.266666666666666, Memory Length: 150000, Optimizer Steps: 1750000, Time Elapsed: 363.1215748786926, Target Q Updates: 175\n",
      "Steps : 1800000, Average Reward: 21.32894736842105, Memory Length: 150000, Optimizer Steps: 1800000, Time Elapsed: 363.80019330978394, Target Q Updates: 180\n",
      "Steps : 1850000, Average Reward: 22.830985915492956, Memory Length: 150000, Optimizer Steps: 1850000, Time Elapsed: 362.9103834629059, Target Q Updates: 185\n",
      "Steps : 1900000, Average Reward: 21.39189189189189, Memory Length: 150000, Optimizer Steps: 1900000, Time Elapsed: 364.44277787208557, Target Q Updates: 190\n",
      "Steps : 1950000, Average Reward: 21.589041095890412, Memory Length: 150000, Optimizer Steps: 1950000, Time Elapsed: 363.4268538951874, Target Q Updates: 195\n",
      "Steps : 2000000, Average Reward: 22.52857142857143, Memory Length: 150000, Optimizer Steps: 2000000, Time Elapsed: 363.0565164089203, Target Q Updates: 200\n",
      "Episode 4905: Saved model weights and log.\n",
      "Steps : 2050000, Average Reward: 21.743243243243242, Memory Length: 150000, Optimizer Steps: 2050000, Time Elapsed: 364.06731057167053, Target Q Updates: 205\n",
      "Steps : 2100000, Average Reward: 22.704225352112676, Memory Length: 150000, Optimizer Steps: 2100000, Time Elapsed: 362.9774441719055, Target Q Updates: 210\n",
      "Steps : 2150000, Average Reward: 22.180555555555557, Memory Length: 150000, Optimizer Steps: 2150000, Time Elapsed: 363.55997490882874, Target Q Updates: 215\n",
      "Steps : 2200000, Average Reward: 23.865671641791046, Memory Length: 150000, Optimizer Steps: 2200000, Time Elapsed: 363.27690029144287, Target Q Updates: 220\n",
      "Steps : 2250000, Average Reward: 23.573529411764707, Memory Length: 150000, Optimizer Steps: 2250000, Time Elapsed: 362.90637946128845, Target Q Updates: 225\n",
      "Steps : 2300000, Average Reward: 23.44776119402985, Memory Length: 150000, Optimizer Steps: 2300000, Time Elapsed: 363.61802768707275, Target Q Updates: 230\n",
      "Steps : 2350000, Average Reward: 24.746268656716417, Memory Length: 150000, Optimizer Steps: 2350000, Time Elapsed: 362.37789821624756, Target Q Updates: 235\n",
      "Steps : 2400000, Average Reward: 25.439393939393938, Memory Length: 150000, Optimizer Steps: 2400000, Time Elapsed: 363.6580641269684, Target Q Updates: 240\n",
      "Steps : 2450000, Average Reward: 26.901639344262296, Memory Length: 150000, Optimizer Steps: 2450000, Time Elapsed: 363.1916391849518, Target Q Updates: 245\n",
      "Steps : 2500000, Average Reward: 25.37878787878788, Memory Length: 150000, Optimizer Steps: 2500000, Time Elapsed: 363.76015734672546, Target Q Updates: 250\n",
      "Episode 5584: Saved model weights and log.\n",
      "Steps : 2550000, Average Reward: 24.985294117647058, Memory Length: 150000, Optimizer Steps: 2550000, Time Elapsed: 364.21957445144653, Target Q Updates: 255\n",
      "Steps : 2600000, Average Reward: 25.676923076923078, Memory Length: 150000, Optimizer Steps: 2600000, Time Elapsed: 362.69218492507935, Target Q Updates: 260\n",
      "Steps : 2650000, Average Reward: 23.04225352112676, Memory Length: 150000, Optimizer Steps: 2650000, Time Elapsed: 362.87134766578674, Target Q Updates: 265\n",
      "Steps : 2700000, Average Reward: 22.814285714285713, Memory Length: 150000, Optimizer Steps: 2700000, Time Elapsed: 362.49100160598755, Target Q Updates: 270\n",
      "Steps : 2750000, Average Reward: 21.32857142857143, Memory Length: 150000, Optimizer Steps: 2750000, Time Elapsed: 362.44395875930786, Target Q Updates: 275\n",
      "Steps : 2800000, Average Reward: 23.428571428571427, Memory Length: 150000, Optimizer Steps: 2800000, Time Elapsed: 362.8272862434387, Target Q Updates: 280\n",
      "Steps : 2850000, Average Reward: 24.441176470588236, Memory Length: 150000, Optimizer Steps: 2850000, Time Elapsed: 362.72354316711426, Target Q Updates: 285\n",
      "Steps : 2900000, Average Reward: 24.434782608695652, Memory Length: 150000, Optimizer Steps: 2900000, Time Elapsed: 362.9946594238281, Target Q Updates: 290\n",
      "Steps : 2950000, Average Reward: 25.37313432835821, Memory Length: 150000, Optimizer Steps: 2950000, Time Elapsed: 362.11165618896484, Target Q Updates: 295\n",
      "Steps : 3000000, Average Reward: 25.294117647058822, Memory Length: 150000, Optimizer Steps: 3000000, Time Elapsed: 362.6921854019165, Target Q Updates: 300\n",
      "Episode 6270: Saved model weights and log.\n",
      "Steps : 3050000, Average Reward: 23.225352112676056, Memory Length: 150000, Optimizer Steps: 3050000, Time Elapsed: 363.81526279449463, Target Q Updates: 305\n",
      "Steps : 3100000, Average Reward: 24.957142857142856, Memory Length: 150000, Optimizer Steps: 3100000, Time Elapsed: 362.5540587902069, Target Q Updates: 310\n",
      "Steps : 3150000, Average Reward: 22.89189189189189, Memory Length: 150000, Optimizer Steps: 3150000, Time Elapsed: 363.7721679210663, Target Q Updates: 315\n",
      "Steps : 3200000, Average Reward: 23.583333333333332, Memory Length: 150000, Optimizer Steps: 3200000, Time Elapsed: 362.86233949661255, Target Q Updates: 320\n",
      "Steps : 3250000, Average Reward: 22.716216216216218, Memory Length: 150000, Optimizer Steps: 3250000, Time Elapsed: 362.790274143219, Target Q Updates: 325\n",
      "Steps : 3300000, Average Reward: 22.2, Memory Length: 150000, Optimizer Steps: 3300000, Time Elapsed: 363.84539675712585, Target Q Updates: 330\n",
      "Steps : 3350000, Average Reward: 23.35135135135135, Memory Length: 150000, Optimizer Steps: 3350000, Time Elapsed: 362.9884543418884, Target Q Updates: 335\n",
      "Steps : 3400000, Average Reward: 22.466666666666665, Memory Length: 150000, Optimizer Steps: 3400000, Time Elapsed: 363.9082918167114, Target Q Updates: 340\n",
      "Steps : 3450000, Average Reward: 23.415584415584416, Memory Length: 150000, Optimizer Steps: 3450000, Time Elapsed: 364.0033781528473, Target Q Updates: 345\n",
      "Steps : 3500000, Average Reward: 22.0, Memory Length: 150000, Optimizer Steps: 3500000, Time Elapsed: 363.6880912780762, Target Q Updates: 350\n",
      "Episode 7010: Saved model weights and log.\n",
      "Steps : 3550000, Average Reward: 22.037974683544302, Memory Length: 150000, Optimizer Steps: 3550000, Time Elapsed: 363.9833605289459, Target Q Updates: 355\n",
      "Steps : 3600000, Average Reward: 23.376623376623378, Memory Length: 150000, Optimizer Steps: 3600000, Time Elapsed: 364.20456099510193, Target Q Updates: 360\n",
      "Steps : 3650000, Average Reward: 22.973684210526315, Memory Length: 150000, Optimizer Steps: 3650000, Time Elapsed: 364.8343117237091, Target Q Updates: 365\n",
      "Steps : 3700000, Average Reward: 21.325301204819276, Memory Length: 150000, Optimizer Steps: 3700000, Time Elapsed: 362.95242166519165, Target Q Updates: 370\n",
      "Steps : 3750000, Average Reward: 24.243243243243242, Memory Length: 150000, Optimizer Steps: 3750000, Time Elapsed: 365.3344473838806, Target Q Updates: 375\n",
      "Steps : 3800000, Average Reward: 23.454545454545453, Memory Length: 150000, Optimizer Steps: 3800000, Time Elapsed: 365.31357073783875, Target Q Updates: 380\n",
      "Steps : 3850000, Average Reward: 23.4625, Memory Length: 150000, Optimizer Steps: 3850000, Time Elapsed: 364.48781967163086, Target Q Updates: 385\n",
      "Steps : 3900000, Average Reward: 23.6125, Memory Length: 150000, Optimizer Steps: 3900000, Time Elapsed: 367.1832981109619, Target Q Updates: 390\n",
      "Steps : 3950000, Average Reward: 23.025, Memory Length: 150000, Optimizer Steps: 3950000, Time Elapsed: 367.190279006958, Target Q Updates: 395\n",
      "Steps : 4000000, Average Reward: 23.289156626506024, Memory Length: 150000, Optimizer Steps: 4000000, Time Elapsed: 364.7830882072449, Target Q Updates: 400\n",
      "Episode 7799: Saved model weights and log.\n",
      "Steps : 4050000, Average Reward: 23.576470588235296, Memory Length: 150000, Optimizer Steps: 4050000, Time Elapsed: 363.7850022315979, Target Q Updates: 405\n",
      "Steps : 4100000, Average Reward: 23.53488372093023, Memory Length: 150000, Optimizer Steps: 4100000, Time Elapsed: 363.2356798648834, Target Q Updates: 410\n",
      "Steps : 4150000, Average Reward: 24.586206896551722, Memory Length: 150000, Optimizer Steps: 4150000, Time Elapsed: 364.02840065956116, Target Q Updates: 415\n",
      "Steps : 4200000, Average Reward: 23.50588235294118, Memory Length: 150000, Optimizer Steps: 4200000, Time Elapsed: 363.76816415786743, Target Q Updates: 420\n",
      "Steps : 4250000, Average Reward: 23.662790697674417, Memory Length: 150000, Optimizer Steps: 4250000, Time Elapsed: 363.51893758773804, Target Q Updates: 425\n",
      "Steps : 4300000, Average Reward: 23.802197802197803, Memory Length: 150000, Optimizer Steps: 4300000, Time Elapsed: 364.85715532302856, Target Q Updates: 430\n",
      "Steps : 4350000, Average Reward: 24.546511627906977, Memory Length: 150000, Optimizer Steps: 4350000, Time Elapsed: 362.75123858451843, Target Q Updates: 435\n",
      "Steps : 4400000, Average Reward: 23.977011494252874, Memory Length: 150000, Optimizer Steps: 4400000, Time Elapsed: 363.2526948451996, Target Q Updates: 440\n",
      "Steps : 4450000, Average Reward: 23.20689655172414, Memory Length: 150000, Optimizer Steps: 4450000, Time Elapsed: 363.02749013900757, Target Q Updates: 445\n",
      "Steps : 4500000, Average Reward: 24.149425287356323, Memory Length: 150000, Optimizer Steps: 4500000, Time Elapsed: 363.2086544036865, Target Q Updates: 450\n",
      "Episode 8666: Saved model weights and log.\n",
      "Steps : 4550000, Average Reward: 25.211111111111112, Memory Length: 150000, Optimizer Steps: 4550000, Time Elapsed: 364.5071334838867, Target Q Updates: 455\n",
      "Steps : 4600000, Average Reward: 25.808988764044944, Memory Length: 150000, Optimizer Steps: 4600000, Time Elapsed: 363.10756254196167, Target Q Updates: 460\n",
      "Steps : 4650000, Average Reward: 26.666666666666668, Memory Length: 150000, Optimizer Steps: 4650000, Time Elapsed: 364.38472533226013, Target Q Updates: 465\n",
      "Steps : 4700000, Average Reward: 27.67816091954023, Memory Length: 150000, Optimizer Steps: 4700000, Time Elapsed: 363.43832182884216, Target Q Updates: 470\n",
      "Steps : 4750000, Average Reward: 26.744444444444444, Memory Length: 150000, Optimizer Steps: 4750000, Time Elapsed: 363.83222246170044, Target Q Updates: 475\n",
      "Steps : 4800000, Average Reward: 26.650602409638555, Memory Length: 150000, Optimizer Steps: 4800000, Time Elapsed: 363.88226795196533, Target Q Updates: 480\n",
      "Steps : 4850000, Average Reward: 26.58888888888889, Memory Length: 150000, Optimizer Steps: 4850000, Time Elapsed: 362.39391350746155, Target Q Updates: 485\n",
      "Steps : 4900000, Average Reward: 28.6, Memory Length: 150000, Optimizer Steps: 4900000, Time Elapsed: 364.64095854759216, Target Q Updates: 490\n",
      "Steps : 4950000, Average Reward: 31.011363636363637, Memory Length: 150000, Optimizer Steps: 4950000, Time Elapsed: 363.9423232078552, Target Q Updates: 495\n",
      "Steps : 5000000, Average Reward: 29.771084337349397, Memory Length: 150000, Optimizer Steps: 5000000, Time Elapsed: 363.7881820201874, Target Q Updates: 500\n",
      "Episode 9546: Saved model weights and log.\n",
      "Steps : 5050000, Average Reward: 31.071428571428573, Memory Length: 150000, Optimizer Steps: 5050000, Time Elapsed: 364.1805396080017, Target Q Updates: 505\n",
      "Steps : 5100000, Average Reward: 32.56818181818182, Memory Length: 150000, Optimizer Steps: 5100000, Time Elapsed: 363.08354115486145, Target Q Updates: 510\n",
      "Steps : 5150000, Average Reward: 30.988235294117647, Memory Length: 150000, Optimizer Steps: 5150000, Time Elapsed: 364.55187797546387, Target Q Updates: 515\n",
      "Steps : 5200000, Average Reward: 29.053763440860216, Memory Length: 150000, Optimizer Steps: 5200000, Time Elapsed: 363.76215863227844, Target Q Updates: 520\n",
      "Steps : 5250000, Average Reward: 32.72093023255814, Memory Length: 150000, Optimizer Steps: 5250000, Time Elapsed: 363.688090801239, Target Q Updates: 525\n",
      "Steps : 5300000, Average Reward: 32.46590909090909, Memory Length: 150000, Optimizer Steps: 5300000, Time Elapsed: 363.5849976539612, Target Q Updates: 530\n",
      "Steps : 5350000, Average Reward: 32.2, Memory Length: 150000, Optimizer Steps: 5350000, Time Elapsed: 362.323849439621, Target Q Updates: 535\n",
      "Steps : 5400000, Average Reward: 33.566265060240966, Memory Length: 150000, Optimizer Steps: 5400000, Time Elapsed: 364.0994658470154, Target Q Updates: 540\n",
      "Steps : 5450000, Average Reward: 28.797752808988765, Memory Length: 150000, Optimizer Steps: 5450000, Time Elapsed: 363.5529685020447, Target Q Updates: 545\n",
      "Steps : 5500000, Average Reward: 32.5632183908046, Memory Length: 150000, Optimizer Steps: 5500000, Time Elapsed: 362.54204773902893, Target Q Updates: 550\n",
      "Episode 10414: Saved model weights and log.\n",
      "Steps : 5550000, Average Reward: 32.69767441860465, Memory Length: 150000, Optimizer Steps: 5550000, Time Elapsed: 363.44887351989746, Target Q Updates: 555\n",
      "Steps : 5600000, Average Reward: 33.67816091954023, Memory Length: 150000, Optimizer Steps: 5600000, Time Elapsed: 362.7702558040619, Target Q Updates: 560\n",
      "Steps : 5650000, Average Reward: 34.47727272727273, Memory Length: 150000, Optimizer Steps: 5650000, Time Elapsed: 364.5749156475067, Target Q Updates: 565\n",
      "Steps : 5700000, Average Reward: 33.247191011235955, Memory Length: 150000, Optimizer Steps: 5700000, Time Elapsed: 363.9283096790314, Target Q Updates: 570\n",
      "Steps : 5750000, Average Reward: 31.045454545454547, Memory Length: 150000, Optimizer Steps: 5750000, Time Elapsed: 363.16861867904663, Target Q Updates: 575\n",
      "Steps : 5800000, Average Reward: 33.56043956043956, Memory Length: 150000, Optimizer Steps: 5800000, Time Elapsed: 364.95224118232727, Target Q Updates: 580\n",
      "Steps : 5850000, Average Reward: 30.916666666666668, Memory Length: 150000, Optimizer Steps: 5850000, Time Elapsed: 363.7421405315399, Target Q Updates: 585\n",
      "Steps : 5900000, Average Reward: 33.29032258064516, Memory Length: 150000, Optimizer Steps: 5900000, Time Elapsed: 365.4657094478607, Target Q Updates: 590\n",
      "Steps : 5950000, Average Reward: 35.13483146067416, Memory Length: 150000, Optimizer Steps: 5950000, Time Elapsed: 367.1622574329376, Target Q Updates: 595\n",
      "Steps : 6000000, Average Reward: 32.89247311827957, Memory Length: 150000, Optimizer Steps: 6000000, Time Elapsed: 367.4605255126953, Target Q Updates: 600\n",
      "Episode 11314: Saved model weights and log.\n",
      "Steps : 6050000, Average Reward: 32.02150537634409, Memory Length: 150000, Optimizer Steps: 6050000, Time Elapsed: 369.23923897743225, Target Q Updates: 605\n",
      "Steps : 6100000, Average Reward: 30.92929292929293, Memory Length: 150000, Optimizer Steps: 6100000, Time Elapsed: 363.9563355445862, Target Q Updates: 610\n",
      "Steps : 6150000, Average Reward: 31.789473684210527, Memory Length: 150000, Optimizer Steps: 6150000, Time Elapsed: 363.80920219421387, Target Q Updates: 615\n",
      "Steps : 6200000, Average Reward: 31.867346938775512, Memory Length: 150000, Optimizer Steps: 6200000, Time Elapsed: 362.86934518814087, Target Q Updates: 620\n",
      "Steps : 6250000, Average Reward: 29.79591836734694, Memory Length: 150000, Optimizer Steps: 6250000, Time Elapsed: 363.19164061546326, Target Q Updates: 625\n",
      "Steps : 6300000, Average Reward: 30.375, Memory Length: 150000, Optimizer Steps: 6300000, Time Elapsed: 370.9699342250824, Target Q Updates: 630\n",
      "Steps : 6350000, Average Reward: 34.85263157894737, Memory Length: 150000, Optimizer Steps: 6350000, Time Elapsed: 365.52776622772217, Target Q Updates: 635\n",
      "Steps : 6400000, Average Reward: 33.180851063829785, Memory Length: 150000, Optimizer Steps: 6400000, Time Elapsed: 366.74187111854553, Target Q Updates: 640\n",
      "Steps : 6450000, Average Reward: 32.202127659574465, Memory Length: 150000, Optimizer Steps: 6450000, Time Elapsed: 365.0002861022949, Target Q Updates: 645\n",
      "Steps : 6500000, Average Reward: 32.73118279569893, Memory Length: 150000, Optimizer Steps: 6500000, Time Elapsed: 369.86471462249756, Target Q Updates: 650\n",
      "Episode 12269: Saved model weights and log.\n",
      "Steps : 6550000, Average Reward: 29.693877551020407, Memory Length: 150000, Optimizer Steps: 6550000, Time Elapsed: 367.97099018096924, Target Q Updates: 655\n",
      "Steps : 6600000, Average Reward: 32.21649484536083, Memory Length: 150000, Optimizer Steps: 6600000, Time Elapsed: 366.75488328933716, Target Q Updates: 660\n",
      "Steps : 6650000, Average Reward: 32.244680851063826, Memory Length: 150000, Optimizer Steps: 6650000, Time Elapsed: 379.33333444595337, Target Q Updates: 665\n",
      "Steps : 6700000, Average Reward: 32.82105263157895, Memory Length: 150000, Optimizer Steps: 6700000, Time Elapsed: 366.61875915527344, Target Q Updates: 670\n",
      "Steps : 6750000, Average Reward: 34.302083333333336, Memory Length: 150000, Optimizer Steps: 6750000, Time Elapsed: 365.0156478881836, Target Q Updates: 675\n",
      "Steps : 6800000, Average Reward: 30.8265306122449, Memory Length: 150000, Optimizer Steps: 6800000, Time Elapsed: 365.54578280448914, Target Q Updates: 680\n",
      "Steps : 6850000, Average Reward: 32.666666666666664, Memory Length: 150000, Optimizer Steps: 6850000, Time Elapsed: 366.50165271759033, Target Q Updates: 685\n",
      "Steps : 6900000, Average Reward: 32.93684210526316, Memory Length: 150000, Optimizer Steps: 6900000, Time Elapsed: 366.5096597671509, Target Q Updates: 690\n",
      "Steps : 6950000, Average Reward: 27.447916666666668, Memory Length: 150000, Optimizer Steps: 6950000, Time Elapsed: 366.2454195022583, Target Q Updates: 695\n",
      "Steps : 7000000, Average Reward: 31.96875, Memory Length: 150000, Optimizer Steps: 7000000, Time Elapsed: 366.0312252044678, Target Q Updates: 700\n",
      "Episode 13230: Saved model weights and log.\n",
      "Steps : 7050000, Average Reward: 34.308510638297875, Memory Length: 150000, Optimizer Steps: 7050000, Time Elapsed: 365.4446904659271, Target Q Updates: 705\n",
      "Steps : 7100000, Average Reward: 33.94736842105263, Memory Length: 150000, Optimizer Steps: 7100000, Time Elapsed: 367.194283246994, Target Q Updates: 710\n",
      "Steps : 7150000, Average Reward: 31.917525773195877, Memory Length: 150000, Optimizer Steps: 7150000, Time Elapsed: 365.9111156463623, Target Q Updates: 715\n",
      "Steps : 7200000, Average Reward: 35.80434782608695, Memory Length: 150000, Optimizer Steps: 7200000, Time Elapsed: 364.7060179710388, Target Q Updates: 720\n",
      "Steps : 7250000, Average Reward: 33.810526315789474, Memory Length: 150000, Optimizer Steps: 7250000, Time Elapsed: 365.2174837589264, Target Q Updates: 725\n",
      "Steps : 7300000, Average Reward: 31.418367346938776, Memory Length: 150000, Optimizer Steps: 7300000, Time Elapsed: 363.27671694755554, Target Q Updates: 730\n",
      "Steps : 7350000, Average Reward: 32.96907216494845, Memory Length: 150000, Optimizer Steps: 7350000, Time Elapsed: 364.46780133247375, Target Q Updates: 735\n",
      "Steps : 7400000, Average Reward: 30.41, Memory Length: 150000, Optimizer Steps: 7400000, Time Elapsed: 365.6668930053711, Target Q Updates: 740\n",
      "Steps : 7450000, Average Reward: 31.19191919191919, Memory Length: 150000, Optimizer Steps: 7450000, Time Elapsed: 364.8251259326935, Target Q Updates: 745\n",
      "Steps : 7500000, Average Reward: 31.03, Memory Length: 150000, Optimizer Steps: 7500000, Time Elapsed: 365.679904460907, Target Q Updates: 750\n",
      "Episode 14197: Saved model weights and log.\n",
      "Steps : 7550000, Average Reward: 31.20618556701031, Memory Length: 150000, Optimizer Steps: 7550000, Time Elapsed: 364.55988454818726, Target Q Updates: 755\n",
      "Steps : 7600000, Average Reward: 32.70967741935484, Memory Length: 150000, Optimizer Steps: 7600000, Time Elapsed: 366.4756293296814, Target Q Updates: 760\n",
      "Steps : 7650000, Average Reward: 37.34736842105263, Memory Length: 150000, Optimizer Steps: 7650000, Time Elapsed: 365.8108961582184, Target Q Updates: 765\n",
      "Steps : 7700000, Average Reward: 33.802083333333336, Memory Length: 150000, Optimizer Steps: 7700000, Time Elapsed: 366.4706244468689, Target Q Updates: 770\n",
      "Steps : 7750000, Average Reward: 31.813725490196077, Memory Length: 150000, Optimizer Steps: 7750000, Time Elapsed: 366.61175322532654, Target Q Updates: 775\n",
      "Steps : 7800000, Average Reward: 34.61224489795919, Memory Length: 150000, Optimizer Steps: 7800000, Time Elapsed: 365.91611981391907, Target Q Updates: 780\n",
      "Steps : 7850000, Average Reward: 34.052083333333336, Memory Length: 150000, Optimizer Steps: 7850000, Time Elapsed: 369.2374744415283, Target Q Updates: 785\n",
      "Steps : 7900000, Average Reward: 35.42857142857143, Memory Length: 150000, Optimizer Steps: 7900000, Time Elapsed: 368.3213093280792, Target Q Updates: 790\n",
      "Steps : 7950000, Average Reward: 32.17821782178218, Memory Length: 150000, Optimizer Steps: 7950000, Time Elapsed: 369.84069204330444, Target Q Updates: 795\n",
      "Steps : 8000000, Average Reward: 37.26315789473684, Memory Length: 150000, Optimizer Steps: 8000000, Time Elapsed: 370.26908254623413, Target Q Updates: 800\n",
      "Episode 15168: Saved model weights and log.\n",
      "Steps : 8050000, Average Reward: 30.833333333333332, Memory Length: 150000, Optimizer Steps: 8050000, Time Elapsed: 366.52202320098877, Target Q Updates: 805\n",
      "Steps : 8100000, Average Reward: 38.37234042553192, Memory Length: 150000, Optimizer Steps: 8100000, Time Elapsed: 365.8210332393646, Target Q Updates: 810\n",
      "Steps : 8150000, Average Reward: 36.297872340425535, Memory Length: 150000, Optimizer Steps: 8150000, Time Elapsed: 365.69491815567017, Target Q Updates: 815\n",
      "Steps : 8200000, Average Reward: 30.424242424242426, Memory Length: 150000, Optimizer Steps: 8200000, Time Elapsed: 367.0891876220703, Target Q Updates: 820\n",
      "Steps : 8250000, Average Reward: 36.189473684210526, Memory Length: 150000, Optimizer Steps: 8250000, Time Elapsed: 365.9761743545532, Target Q Updates: 825\n",
      "Steps : 8300000, Average Reward: 32.34653465346535, Memory Length: 150000, Optimizer Steps: 8300000, Time Elapsed: 367.3624265193939, Target Q Updates: 830\n",
      "Steps : 8350000, Average Reward: 32.17171717171717, Memory Length: 150000, Optimizer Steps: 8350000, Time Elapsed: 366.2484223842621, Target Q Updates: 835\n",
      "Steps : 8400000, Average Reward: 33.86868686868687, Memory Length: 150000, Optimizer Steps: 8400000, Time Elapsed: 367.35040616989136, Target Q Updates: 840\n",
      "Steps : 8450000, Average Reward: 31.245098039215687, Memory Length: 150000, Optimizer Steps: 8450000, Time Elapsed: 367.0171227455139, Target Q Updates: 845\n",
      "Steps : 8500000, Average Reward: 29.41176470588235, Memory Length: 150000, Optimizer Steps: 8500000, Time Elapsed: 366.2834532260895, Target Q Updates: 850\n",
      "Episode 16155: Saved model weights and log.\n",
      "Steps : 8550000, Average Reward: 29.745098039215687, Memory Length: 150000, Optimizer Steps: 8550000, Time Elapsed: 367.6295499801636, Target Q Updates: 855\n",
      "Steps : 8600000, Average Reward: 32.88775510204081, Memory Length: 150000, Optimizer Steps: 8600000, Time Elapsed: 367.84083461761475, Target Q Updates: 860\n",
      "Steps : 8650000, Average Reward: 35.255102040816325, Memory Length: 150000, Optimizer Steps: 8650000, Time Elapsed: 368.4063866138458, Target Q Updates: 865\n",
      "Steps : 8700000, Average Reward: 33.21, Memory Length: 150000, Optimizer Steps: 8700000, Time Elapsed: 365.985182762146, Target Q Updates: 870\n",
      "Steps : 8750000, Average Reward: 32.85436893203884, Memory Length: 150000, Optimizer Steps: 8750000, Time Elapsed: 366.1923711299896, Target Q Updates: 875\n",
      "Steps : 8800000, Average Reward: 33.35643564356435, Memory Length: 150000, Optimizer Steps: 8800000, Time Elapsed: 366.9740824699402, Target Q Updates: 880\n",
      "Steps : 8850000, Average Reward: 32.98, Memory Length: 150000, Optimizer Steps: 8850000, Time Elapsed: 366.4816346168518, Target Q Updates: 885\n",
      "Steps : 8900000, Average Reward: 32.584158415841586, Memory Length: 150000, Optimizer Steps: 8900000, Time Elapsed: 367.78081703186035, Target Q Updates: 890\n",
      "Steps : 8950000, Average Reward: 33.97, Memory Length: 150000, Optimizer Steps: 8950000, Time Elapsed: 367.0691695213318, Target Q Updates: 895\n",
      "Steps : 9000000, Average Reward: 34.333333333333336, Memory Length: 150000, Optimizer Steps: 9000000, Time Elapsed: 366.47423934936523, Target Q Updates: 900\n",
      "Episode 17160: Saved model weights and log.\n",
      "Steps : 9050000, Average Reward: 32.09090909090909, Memory Length: 150000, Optimizer Steps: 9050000, Time Elapsed: 367.8869140148163, Target Q Updates: 905\n",
      "Steps : 9100000, Average Reward: 35.885416666666664, Memory Length: 150000, Optimizer Steps: 9100000, Time Elapsed: 366.74887776374817, Target Q Updates: 910\n",
      "Steps : 9150000, Average Reward: 32.07, Memory Length: 150000, Optimizer Steps: 9150000, Time Elapsed: 367.9759945869446, Target Q Updates: 915\n",
      "Steps : 9200000, Average Reward: 33.61224489795919, Memory Length: 150000, Optimizer Steps: 9200000, Time Elapsed: 366.76289081573486, Target Q Updates: 920\n",
      "Steps : 9250000, Average Reward: 34.88421052631579, Memory Length: 150000, Optimizer Steps: 9250000, Time Elapsed: 367.3594334125519, Target Q Updates: 925\n",
      "Steps : 9300000, Average Reward: 31.418367346938776, Memory Length: 150000, Optimizer Steps: 9300000, Time Elapsed: 368.86980867385864, Target Q Updates: 930\n",
      "Steps : 9350000, Average Reward: 33.0, Memory Length: 150000, Optimizer Steps: 9350000, Time Elapsed: 367.7908263206482, Target Q Updates: 935\n",
      "Steps : 9400000, Average Reward: 34.77777777777778, Memory Length: 150000, Optimizer Steps: 9400000, Time Elapsed: 368.37936210632324, Target Q Updates: 940\n",
      "Steps : 9450000, Average Reward: 33.35, Memory Length: 150000, Optimizer Steps: 9450000, Time Elapsed: 367.577632188797, Target Q Updates: 945\n",
      "Steps : 9500000, Average Reward: 34.57575757575758, Memory Length: 150000, Optimizer Steps: 9500000, Time Elapsed: 369.61472272872925, Target Q Updates: 950\n",
      "Episode 18145: Saved model weights and log.\n",
      "Steps : 9550000, Average Reward: 34.8469387755102, Memory Length: 150000, Optimizer Steps: 9550000, Time Elapsed: 377.6919848918915, Target Q Updates: 955\n",
      "Steps : 9600000, Average Reward: 33.97029702970297, Memory Length: 150000, Optimizer Steps: 9600000, Time Elapsed: 386.08674788475037, Target Q Updates: 960\n",
      "Steps : 9650000, Average Reward: 34.62244897959184, Memory Length: 150000, Optimizer Steps: 9650000, Time Elapsed: 375.55752301216125, Target Q Updates: 965\n",
      "Steps : 9700000, Average Reward: 32.62376237623762, Memory Length: 150000, Optimizer Steps: 9700000, Time Elapsed: 376.1073725223541, Target Q Updates: 970\n",
      "Steps : 9750000, Average Reward: 36.642857142857146, Memory Length: 150000, Optimizer Steps: 9750000, Time Elapsed: 371.059974193573, Target Q Updates: 975\n",
      "Steps : 9800000, Average Reward: 31.712871287128714, Memory Length: 150000, Optimizer Steps: 9800000, Time Elapsed: 371.89083433151245, Target Q Updates: 980\n",
      "Steps : 9850000, Average Reward: 33.26732673267327, Memory Length: 150000, Optimizer Steps: 9850000, Time Elapsed: 369.9889268875122, Target Q Updates: 985\n",
      "Steps : 9900000, Average Reward: 31.784313725490197, Memory Length: 150000, Optimizer Steps: 9900000, Time Elapsed: 369.7694659233093, Target Q Updates: 990\n",
      "Steps : 9950000, Average Reward: 31.970588235294116, Memory Length: 150000, Optimizer Steps: 9950000, Time Elapsed: 370.61345887184143, Target Q Updates: 995\n",
      "Steps : 10000000, Average Reward: 32.04807692307692, Memory Length: 150000, Optimizer Steps: 10000000, Time Elapsed: 383.45053815841675, Target Q Updates: 1000\n",
      "Episode 19151: Saved model weights and log.\n",
      "Steps : 10050000, Average Reward: 32.039603960396036, Memory Length: 150000, Optimizer Steps: 10050000, Time Elapsed: 371.617636680603, Target Q Updates: 1005\n",
      "Steps : 10100000, Average Reward: 34.37373737373738, Memory Length: 150000, Optimizer Steps: 10100000, Time Elapsed: 371.8643205165863, Target Q Updates: 1010\n",
      "Steps : 10150000, Average Reward: 27.787037037037038, Memory Length: 150000, Optimizer Steps: 10150000, Time Elapsed: 370.7285108566284, Target Q Updates: 1015\n",
      "Steps : 10200000, Average Reward: 33.72, Memory Length: 150000, Optimizer Steps: 10200000, Time Elapsed: 372.4476580619812, Target Q Updates: 1020\n",
      "Steps : 10250000, Average Reward: 30.068627450980394, Memory Length: 150000, Optimizer Steps: 10250000, Time Elapsed: 380.59130668640137, Target Q Updates: 1025\n",
      "Steps : 10300000, Average Reward: 30.419047619047618, Memory Length: 150000, Optimizer Steps: 10300000, Time Elapsed: 376.92360973358154, Target Q Updates: 1030\n",
      "Steps : 10350000, Average Reward: 33.99009900990099, Memory Length: 150000, Optimizer Steps: 10350000, Time Elapsed: 374.378173828125, Target Q Updates: 1035\n",
      "Steps : 10400000, Average Reward: 26.439252336448597, Memory Length: 150000, Optimizer Steps: 10400000, Time Elapsed: 373.1168677806854, Target Q Updates: 1040\n",
      "Steps : 10450000, Average Reward: 32.310679611650485, Memory Length: 150000, Optimizer Steps: 10450000, Time Elapsed: 371.89603304862976, Target Q Updates: 1045\n",
      "Steps : 10500000, Average Reward: 29.194174757281555, Memory Length: 150000, Optimizer Steps: 10500000, Time Elapsed: 374.40923714637756, Target Q Updates: 1050\n",
      "Episode 20180: Saved model weights and log.\n",
      "Steps : 10550000, Average Reward: 30.07070707070707, Memory Length: 150000, Optimizer Steps: 10550000, Time Elapsed: 372.9231400489807, Target Q Updates: 1055\n",
      "Steps : 10600000, Average Reward: 32.61616161616162, Memory Length: 150000, Optimizer Steps: 10600000, Time Elapsed: 371.7754371166229, Target Q Updates: 1060\n",
      "Steps : 10650000, Average Reward: 29.203883495145632, Memory Length: 150000, Optimizer Steps: 10650000, Time Elapsed: 369.9375183582306, Target Q Updates: 1065\n",
      "Steps : 10700000, Average Reward: 29.2, Memory Length: 150000, Optimizer Steps: 10700000, Time Elapsed: 370.2391576766968, Target Q Updates: 1070\n",
      "Steps : 10750000, Average Reward: 34.59405940594059, Memory Length: 150000, Optimizer Steps: 10750000, Time Elapsed: 369.9670226573944, Target Q Updates: 1075\n",
      "Steps : 10800000, Average Reward: 31.12, Memory Length: 150000, Optimizer Steps: 10800000, Time Elapsed: 372.36822414398193, Target Q Updates: 1080\n",
      "Steps : 10850000, Average Reward: 33.40816326530612, Memory Length: 150000, Optimizer Steps: 10850000, Time Elapsed: 370.75995802879333, Target Q Updates: 1085\n",
      "Steps : 10900000, Average Reward: 27.135922330097088, Memory Length: 150000, Optimizer Steps: 10900000, Time Elapsed: 367.7529902458191, Target Q Updates: 1090\n",
      "Steps : 10950000, Average Reward: 32.05102040816327, Memory Length: 150000, Optimizer Steps: 10950000, Time Elapsed: 371.8798713684082, Target Q Updates: 1095\n",
      "Steps : 11000000, Average Reward: 35.47422680412371, Memory Length: 150000, Optimizer Steps: 11000000, Time Elapsed: 368.7859663963318, Target Q Updates: 1100\n",
      "Episode 21183: Saved model weights and log.\n",
      "Steps : 11050000, Average Reward: 35.854166666666664, Memory Length: 150000, Optimizer Steps: 11050000, Time Elapsed: 368.86719703674316, Target Q Updates: 1105\n",
      "Steps : 11100000, Average Reward: 30.959183673469386, Memory Length: 150000, Optimizer Steps: 11100000, Time Elapsed: 369.03796434402466, Target Q Updates: 1110\n",
      "Steps : 11150000, Average Reward: 33.97979797979798, Memory Length: 150000, Optimizer Steps: 11150000, Time Elapsed: 366.84896874427795, Target Q Updates: 1115\n",
      "Steps : 11200000, Average Reward: 32.306930693069305, Memory Length: 150000, Optimizer Steps: 11200000, Time Elapsed: 366.873991727829, Target Q Updates: 1120\n",
      "Steps : 11250000, Average Reward: 27.952830188679247, Memory Length: 150000, Optimizer Steps: 11250000, Time Elapsed: 368.31230068206787, Target Q Updates: 1125\n",
      "Steps : 11300000, Average Reward: 30.861386138613863, Memory Length: 150000, Optimizer Steps: 11300000, Time Elapsed: 368.6866445541382, Target Q Updates: 1130\n",
      "Steps : 11350000, Average Reward: 28.114285714285714, Memory Length: 150000, Optimizer Steps: 11350000, Time Elapsed: 368.3943758010864, Target Q Updates: 1135\n",
      "Steps : 11400000, Average Reward: 29.30392156862745, Memory Length: 150000, Optimizer Steps: 11400000, Time Elapsed: 366.97482991218567, Target Q Updates: 1140\n",
      "Steps : 11450000, Average Reward: 29.12621359223301, Memory Length: 150000, Optimizer Steps: 11450000, Time Elapsed: 367.5396018028259, Target Q Updates: 1145\n",
      "Steps : 11500000, Average Reward: 34.227722772277225, Memory Length: 150000, Optimizer Steps: 11500000, Time Elapsed: 367.22251176834106, Target Q Updates: 1150\n",
      "Episode 22195: Saved model weights and log.\n",
      "Steps : 11550000, Average Reward: 31.233009708737864, Memory Length: 150000, Optimizer Steps: 11550000, Time Elapsed: 368.28300046920776, Target Q Updates: 1155\n",
      "Steps : 11600000, Average Reward: 30.27358490566038, Memory Length: 150000, Optimizer Steps: 11600000, Time Elapsed: 368.68430161476135, Target Q Updates: 1160\n",
      "Steps : 11650000, Average Reward: 29.160377358490567, Memory Length: 150000, Optimizer Steps: 11650000, Time Elapsed: 367.0428891181946, Target Q Updates: 1165\n",
      "Steps : 11700000, Average Reward: 27.403846153846153, Memory Length: 150000, Optimizer Steps: 11700000, Time Elapsed: 367.56161761283875, Target Q Updates: 1170\n",
      "Steps : 11750000, Average Reward: 35.1530612244898, Memory Length: 150000, Optimizer Steps: 11750000, Time Elapsed: 367.480544090271, Target Q Updates: 1175\n",
      "Steps : 11800000, Average Reward: 30.45631067961165, Memory Length: 150000, Optimizer Steps: 11800000, Time Elapsed: 366.8389620780945, Target Q Updates: 1180\n",
      "Steps : 11850000, Average Reward: 32.79207920792079, Memory Length: 150000, Optimizer Steps: 11850000, Time Elapsed: 368.0800895690918, Target Q Updates: 1185\n",
      "Steps : 11900000, Average Reward: 31.554455445544555, Memory Length: 150000, Optimizer Steps: 11900000, Time Elapsed: 366.7068395614624, Target Q Updates: 1190\n",
      "Steps : 11950000, Average Reward: 35.53061224489796, Memory Length: 150000, Optimizer Steps: 11950000, Time Elapsed: 367.0250463485718, Target Q Updates: 1195\n",
      "Steps : 12000000, Average Reward: 31.141414141414142, Memory Length: 150000, Optimizer Steps: 12000000, Time Elapsed: 368.050062417984, Target Q Updates: 1200\n",
      "Episode 23214: Saved model weights and log.\n",
      "Steps : 12050000, Average Reward: 30.412371134020617, Memory Length: 150000, Optimizer Steps: 12050000, Time Elapsed: 369.08900809288025, Target Q Updates: 1205\n",
      "Steps : 12100000, Average Reward: 28.443396226415093, Memory Length: 150000, Optimizer Steps: 12100000, Time Elapsed: 375.29209446907043, Target Q Updates: 1210\n",
      "Steps : 12150000, Average Reward: 32.37373737373738, Memory Length: 150000, Optimizer Steps: 12150000, Time Elapsed: 368.5715367794037, Target Q Updates: 1215\n",
      "Steps : 12200000, Average Reward: 33.42, Memory Length: 150000, Optimizer Steps: 12200000, Time Elapsed: 367.90616178512573, Target Q Updates: 1220\n",
      "Steps : 12250000, Average Reward: 30.722772277227723, Memory Length: 150000, Optimizer Steps: 12250000, Time Elapsed: 368.2681667804718, Target Q Updates: 1225\n",
      "Steps : 12300000, Average Reward: 29.02803738317757, Memory Length: 150000, Optimizer Steps: 12300000, Time Elapsed: 367.3764522075653, Target Q Updates: 1230\n",
      "Steps : 12350000, Average Reward: 32.05, Memory Length: 150000, Optimizer Steps: 12350000, Time Elapsed: 368.7221474647522, Target Q Updates: 1235\n",
      "Steps : 12400000, Average Reward: 29.153846153846153, Memory Length: 150000, Optimizer Steps: 12400000, Time Elapsed: 366.300240278244, Target Q Updates: 1240\n",
      "Steps : 12450000, Average Reward: 29.686274509803923, Memory Length: 150000, Optimizer Steps: 12450000, Time Elapsed: 367.61321568489075, Target Q Updates: 1245\n",
      "Steps : 12500000, Average Reward: 32.15841584158416, Memory Length: 150000, Optimizer Steps: 12500000, Time Elapsed: 367.5315902233124, Target Q Updates: 1250\n",
      "Episode 24231: Saved model weights and log.\n",
      "Steps : 12550000, Average Reward: 30.163265306122447, Memory Length: 150000, Optimizer Steps: 12550000, Time Elapsed: 367.85788321495056, Target Q Updates: 1255\n",
      "Steps : 12600000, Average Reward: 28.42, Memory Length: 150000, Optimizer Steps: 12600000, Time Elapsed: 368.7537021636963, Target Q Updates: 1260\n",
      "Steps : 12650000, Average Reward: 29.676470588235293, Memory Length: 150000, Optimizer Steps: 12650000, Time Elapsed: 368.1791799068451, Target Q Updates: 1265\n",
      "Steps : 12700000, Average Reward: 31.366336633663366, Memory Length: 150000, Optimizer Steps: 12700000, Time Elapsed: 368.75210428237915, Target Q Updates: 1270\n",
      "Steps : 12750000, Average Reward: 33.35, Memory Length: 150000, Optimizer Steps: 12750000, Time Elapsed: 366.0502419471741, Target Q Updates: 1275\n",
      "Steps : 12800000, Average Reward: 32.07920792079208, Memory Length: 150000, Optimizer Steps: 12800000, Time Elapsed: 367.0781786441803, Target Q Updates: 1280\n",
      "Steps : 12850000, Average Reward: 33.08080808080808, Memory Length: 150000, Optimizer Steps: 12850000, Time Elapsed: 367.9509711265564, Target Q Updates: 1285\n",
      "Steps : 12900000, Average Reward: 29.728155339805824, Memory Length: 150000, Optimizer Steps: 12900000, Time Elapsed: 371.0728142261505, Target Q Updates: 1290\n",
      "Steps : 12950000, Average Reward: 29.901960784313726, Memory Length: 150000, Optimizer Steps: 12950000, Time Elapsed: 369.20110988616943, Target Q Updates: 1295\n",
      "Steps : 13000000, Average Reward: 30.58823529411765, Memory Length: 150000, Optimizer Steps: 13000000, Time Elapsed: 367.91393876075745, Target Q Updates: 1300\n",
      "Episode 25239: Saved model weights and log.\n",
      "Steps : 13050000, Average Reward: 32.86, Memory Length: 150000, Optimizer Steps: 13050000, Time Elapsed: 368.1307735443115, Target Q Updates: 1305\n",
      "Steps : 13100000, Average Reward: 32.8921568627451, Memory Length: 150000, Optimizer Steps: 13100000, Time Elapsed: 369.1270432472229, Target Q Updates: 1310\n",
      "Steps : 13150000, Average Reward: 33.65384615384615, Memory Length: 150000, Optimizer Steps: 13150000, Time Elapsed: 368.42440485954285, Target Q Updates: 1315\n",
      "Steps : 13200000, Average Reward: 30.637254901960784, Memory Length: 150000, Optimizer Steps: 13200000, Time Elapsed: 369.39655113220215, Target Q Updates: 1320\n",
      "Steps : 13250000, Average Reward: 28.75, Memory Length: 150000, Optimizer Steps: 13250000, Time Elapsed: 367.857887506485, Target Q Updates: 1325\n",
      "Steps : 13300000, Average Reward: 32.07920792079208, Memory Length: 150000, Optimizer Steps: 13300000, Time Elapsed: 367.577632188797, Target Q Updates: 1330\n",
      "Steps : 13350000, Average Reward: 33.86734693877551, Memory Length: 150000, Optimizer Steps: 13350000, Time Elapsed: 368.8289577960968, Target Q Updates: 1335\n",
      "Steps : 13400000, Average Reward: 36.44329896907217, Memory Length: 150000, Optimizer Steps: 13400000, Time Elapsed: 367.7287697792053, Target Q Updates: 1340\n",
      "Steps : 13450000, Average Reward: 27.577981651376145, Memory Length: 150000, Optimizer Steps: 13450000, Time Elapsed: 369.07899951934814, Target Q Updates: 1345\n",
      "Steps : 13500000, Average Reward: 33.349514563106794, Memory Length: 150000, Optimizer Steps: 13500000, Time Elapsed: 367.8729009628296, Target Q Updates: 1350\n",
      "Episode 26259: Saved model weights and log.\n",
      "Steps : 13550000, Average Reward: 33.8921568627451, Memory Length: 150000, Optimizer Steps: 13550000, Time Elapsed: 368.95081901550293, Target Q Updates: 1355\n",
      "Steps : 13600000, Average Reward: 27.990384615384617, Memory Length: 150000, Optimizer Steps: 13600000, Time Elapsed: 368.66462206840515, Target Q Updates: 1360\n",
      "Steps : 13650000, Average Reward: 32.54455445544554, Memory Length: 150000, Optimizer Steps: 13650000, Time Elapsed: 367.8638925552368, Target Q Updates: 1365\n",
      "Steps : 13700000, Average Reward: 26.824074074074073, Memory Length: 150000, Optimizer Steps: 13700000, Time Elapsed: 368.5605399608612, Target Q Updates: 1370\n",
      "Steps : 13750000, Average Reward: 27.813084112149532, Memory Length: 150000, Optimizer Steps: 13750000, Time Elapsed: 366.71407771110535, Target Q Updates: 1375\n",
      "Steps : 13800000, Average Reward: 33.61616161616162, Memory Length: 150000, Optimizer Steps: 13800000, Time Elapsed: 368.726678609848, Target Q Updates: 1380\n",
      "Steps : 13850000, Average Reward: 39.202127659574465, Memory Length: 150000, Optimizer Steps: 13850000, Time Elapsed: 368.1031107902527, Target Q Updates: 1385\n",
      "Steps : 13900000, Average Reward: 34.33673469387755, Memory Length: 150000, Optimizer Steps: 13900000, Time Elapsed: 366.70483660697937, Target Q Updates: 1390\n",
      "Steps : 13950000, Average Reward: 30.07920792079208, Memory Length: 150000, Optimizer Steps: 13950000, Time Elapsed: 368.2170741558075, Target Q Updates: 1395\n",
      "Steps : 14000000, Average Reward: 30.067307692307693, Memory Length: 150000, Optimizer Steps: 14000000, Time Elapsed: 366.7460904121399, Target Q Updates: 1400\n",
      "Episode 27277: Saved model weights and log.\n",
      "Steps : 14050000, Average Reward: 37.242105263157896, Memory Length: 150000, Optimizer Steps: 14050000, Time Elapsed: 367.7537775039673, Target Q Updates: 1405\n",
      "Steps : 14100000, Average Reward: 33.0, Memory Length: 150000, Optimizer Steps: 14100000, Time Elapsed: 367.9539747238159, Target Q Updates: 1410\n",
      "Steps : 14150000, Average Reward: 33.22222222222222, Memory Length: 150000, Optimizer Steps: 14150000, Time Elapsed: 367.7437837123871, Target Q Updates: 1415\n",
      "Steps : 14200000, Average Reward: 31.225490196078432, Memory Length: 150000, Optimizer Steps: 14200000, Time Elapsed: 368.70666003227234, Target Q Updates: 1420\n",
      "Steps : 14250000, Average Reward: 29.745098039215687, Memory Length: 150000, Optimizer Steps: 14250000, Time Elapsed: 368.09910702705383, Target Q Updates: 1425\n",
      "Steps : 14300000, Average Reward: 32.52427184466019, Memory Length: 150000, Optimizer Steps: 14300000, Time Elapsed: 367.85688614845276, Target Q Updates: 1430\n",
      "Steps : 14350000, Average Reward: 27.145631067961165, Memory Length: 150000, Optimizer Steps: 14350000, Time Elapsed: 368.5815465450287, Target Q Updates: 1435\n",
      "Steps : 14400000, Average Reward: 31.0, Memory Length: 150000, Optimizer Steps: 14400000, Time Elapsed: 368.2942843437195, Target Q Updates: 1440\n",
      "Steps : 14450000, Average Reward: 31.683168316831683, Memory Length: 150000, Optimizer Steps: 14450000, Time Elapsed: 368.8131926059723, Target Q Updates: 1445\n",
      "Steps : 14500000, Average Reward: 37.84375, Memory Length: 150000, Optimizer Steps: 14500000, Time Elapsed: 367.53459310531616, Target Q Updates: 1450\n",
      "Episode 28278: Saved model weights and log.\n",
      "Steps : 14550000, Average Reward: 36.56565656565657, Memory Length: 150000, Optimizer Steps: 14550000, Time Elapsed: 369.69856357574463, Target Q Updates: 1455\n",
      "Steps : 14600000, Average Reward: 34.505050505050505, Memory Length: 150000, Optimizer Steps: 14600000, Time Elapsed: 368.9408736228943, Target Q Updates: 1460\n",
      "Steps : 14650000, Average Reward: 30.20952380952381, Memory Length: 150000, Optimizer Steps: 14650000, Time Elapsed: 367.9429647922516, Target Q Updates: 1465\n",
      "Steps : 14700000, Average Reward: 32.08, Memory Length: 150000, Optimizer Steps: 14700000, Time Elapsed: 369.16007256507874, Target Q Updates: 1470\n",
      "Steps : 14750000, Average Reward: 36.78787878787879, Memory Length: 150000, Optimizer Steps: 14750000, Time Elapsed: 367.5235834121704, Target Q Updates: 1475\n",
      "Steps : 14800000, Average Reward: 30.99019607843137, Memory Length: 150000, Optimizer Steps: 14800000, Time Elapsed: 367.87290382385254, Target Q Updates: 1480\n",
      "Steps : 14850000, Average Reward: 30.009615384615383, Memory Length: 150000, Optimizer Steps: 14850000, Time Elapsed: 368.41039061546326, Target Q Updates: 1485\n",
      "Steps : 14900000, Average Reward: 28.401869158878505, Memory Length: 150000, Optimizer Steps: 14900000, Time Elapsed: 367.84172010421753, Target Q Updates: 1490\n",
      "Steps : 14950000, Average Reward: 34.58163265306123, Memory Length: 150000, Optimizer Steps: 14950000, Time Elapsed: 367.60665798187256, Target Q Updates: 1495\n",
      "Steps : 15000000, Average Reward: 29.78846153846154, Memory Length: 150000, Optimizer Steps: 15000000, Time Elapsed: 367.03513836860657, Target Q Updates: 1500\n",
      "Episode 29295: Saved model weights and log.\n",
      "Steps : 15050000, Average Reward: 28.33009708737864, Memory Length: 150000, Optimizer Steps: 15050000, Time Elapsed: 368.49147629737854, Target Q Updates: 1505\n",
      "Steps : 15100000, Average Reward: 30.87878787878788, Memory Length: 150000, Optimizer Steps: 15100000, Time Elapsed: 366.8369576931, Target Q Updates: 1510\n",
      "Steps : 15150000, Average Reward: 39.37362637362637, Memory Length: 150000, Optimizer Steps: 15150000, Time Elapsed: 367.7487881183624, Target Q Updates: 1515\n",
      "Steps : 15200000, Average Reward: 32.009803921568626, Memory Length: 150000, Optimizer Steps: 15200000, Time Elapsed: 367.099196434021, Target Q Updates: 1520\n",
      "Steps : 15250000, Average Reward: 38.577319587628864, Memory Length: 150000, Optimizer Steps: 15250000, Time Elapsed: 366.8829998970032, Target Q Updates: 1525\n",
      "Steps : 15300000, Average Reward: 31.137254901960784, Memory Length: 150000, Optimizer Steps: 15300000, Time Elapsed: 367.781818151474, Target Q Updates: 1530\n",
      "Steps : 15350000, Average Reward: 35.30927835051546, Memory Length: 150000, Optimizer Steps: 15350000, Time Elapsed: 367.39746832847595, Target Q Updates: 1535\n",
      "Steps : 15400000, Average Reward: 34.25, Memory Length: 150000, Optimizer Steps: 15400000, Time Elapsed: 366.8789987564087, Target Q Updates: 1540\n",
      "Steps : 15450000, Average Reward: 37.59782608695652, Memory Length: 150000, Optimizer Steps: 15450000, Time Elapsed: 369.2874746322632, Target Q Updates: 1545\n",
      "Steps : 15500000, Average Reward: 31.0, Memory Length: 150000, Optimizer Steps: 15500000, Time Elapsed: 369.36726117134094, Target Q Updates: 1550\n",
      "Episode 30274: Saved model weights and log.\n",
      "Steps : 15550000, Average Reward: 32.322916666666664, Memory Length: 150000, Optimizer Steps: 15550000, Time Elapsed: 368.572550535202, Target Q Updates: 1555\n",
      "Steps : 15600000, Average Reward: 36.21212121212121, Memory Length: 150000, Optimizer Steps: 15600000, Time Elapsed: 367.3494246006012, Target Q Updates: 1560\n",
      "Steps : 15650000, Average Reward: 37.97938144329897, Memory Length: 150000, Optimizer Steps: 15650000, Time Elapsed: 366.87499260902405, Target Q Updates: 1565\n",
      "Steps : 15700000, Average Reward: 36.747368421052634, Memory Length: 150000, Optimizer Steps: 15700000, Time Elapsed: 368.0150330066681, Target Q Updates: 1570\n",
      "Steps : 15750000, Average Reward: 38.58510638297872, Memory Length: 150000, Optimizer Steps: 15750000, Time Elapsed: 368.0190341472626, Target Q Updates: 1575\n",
      "Steps : 15800000, Average Reward: 32.07920792079208, Memory Length: 150000, Optimizer Steps: 15800000, Time Elapsed: 368.0380516052246, Target Q Updates: 1580\n",
      "Steps : 15850000, Average Reward: 35.20618556701031, Memory Length: 150000, Optimizer Steps: 15850000, Time Elapsed: 366.95906829833984, Target Q Updates: 1585\n",
      "Steps : 15900000, Average Reward: 37.54, Memory Length: 150000, Optimizer Steps: 15900000, Time Elapsed: 367.1192150115967, Target Q Updates: 1590\n",
      "Steps : 15950000, Average Reward: 32.525252525252526, Memory Length: 150000, Optimizer Steps: 15950000, Time Elapsed: 368.22542452812195, Target Q Updates: 1595\n",
      "Steps : 16000000, Average Reward: 38.229166666666664, Memory Length: 150000, Optimizer Steps: 16000000, Time Elapsed: 367.0531544685364, Target Q Updates: 1600\n",
      "Episode 31248: Saved model weights and log.\n",
      "Steps : 16050000, Average Reward: 37.40625, Memory Length: 150000, Optimizer Steps: 16050000, Time Elapsed: 372.74797654151917, Target Q Updates: 1605\n",
      "Steps : 16100000, Average Reward: 33.91836734693877, Memory Length: 150000, Optimizer Steps: 16100000, Time Elapsed: 369.7215836048126, Target Q Updates: 1610\n",
      "Steps : 16150000, Average Reward: 38.366336633663366, Memory Length: 150000, Optimizer Steps: 16150000, Time Elapsed: 367.3734471797943, Target Q Updates: 1615\n",
      "Steps : 16200000, Average Reward: 28.72222222222222, Memory Length: 150000, Optimizer Steps: 16200000, Time Elapsed: 368.3173053264618, Target Q Updates: 1620\n",
      "Steps : 16250000, Average Reward: 32.794117647058826, Memory Length: 150000, Optimizer Steps: 16250000, Time Elapsed: 367.0861847400665, Target Q Updates: 1625\n",
      "Steps : 16300000, Average Reward: 35.474747474747474, Memory Length: 150000, Optimizer Steps: 16300000, Time Elapsed: 367.7207627296448, Target Q Updates: 1630\n",
      "Steps : 16350000, Average Reward: 37.30612244897959, Memory Length: 150000, Optimizer Steps: 16350000, Time Elapsed: 367.5005552768707, Target Q Updates: 1635\n",
      "Steps : 16400000, Average Reward: 28.321100917431192, Memory Length: 150000, Optimizer Steps: 16400000, Time Elapsed: 367.3351812362671, Target Q Updates: 1640\n",
      "Steps : 16450000, Average Reward: 35.33, Memory Length: 150000, Optimizer Steps: 16450000, Time Elapsed: 366.72585701942444, Target Q Updates: 1645\n",
      "Steps : 16500000, Average Reward: 30.429906542056074, Memory Length: 150000, Optimizer Steps: 16500000, Time Elapsed: 366.4526102542877, Target Q Updates: 1650\n",
      "Episode 32266: Saved model weights and log.\n",
      "Steps : 16550000, Average Reward: 33.73737373737374, Memory Length: 150000, Optimizer Steps: 16550000, Time Elapsed: 366.16134309768677, Target Q Updates: 1655\n",
      "Steps : 16600000, Average Reward: 28.695238095238096, Memory Length: 150000, Optimizer Steps: 16600000, Time Elapsed: 366.6097538471222, Target Q Updates: 1660\n",
      "Steps : 16650000, Average Reward: 33.21153846153846, Memory Length: 150000, Optimizer Steps: 16650000, Time Elapsed: 365.54978585243225, Target Q Updates: 1665\n",
      "Steps : 16700000, Average Reward: 37.1578947368421, Memory Length: 150000, Optimizer Steps: 16700000, Time Elapsed: 366.6390233039856, Target Q Updates: 1670\n",
      "Steps : 16750000, Average Reward: 29.37864077669903, Memory Length: 150000, Optimizer Steps: 16750000, Time Elapsed: 366.77890491485596, Target Q Updates: 1675\n",
      "Steps : 16800000, Average Reward: 33.05102040816327, Memory Length: 150000, Optimizer Steps: 16800000, Time Elapsed: 366.99910593032837, Target Q Updates: 1680\n",
      "Steps : 16850000, Average Reward: 30.892156862745097, Memory Length: 150000, Optimizer Steps: 16850000, Time Elapsed: 367.6096613407135, Target Q Updates: 1685\n",
      "Steps : 16900000, Average Reward: 36.20618556701031, Memory Length: 150000, Optimizer Steps: 16900000, Time Elapsed: 366.4656226634979, Target Q Updates: 1690\n",
      "Steps : 16950000, Average Reward: 28.933333333333334, Memory Length: 150000, Optimizer Steps: 16950000, Time Elapsed: 368.07708716392517, Target Q Updates: 1695\n",
      "Steps : 17000000, Average Reward: 35.04040404040404, Memory Length: 150000, Optimizer Steps: 17000000, Time Elapsed: 365.98518204689026, Target Q Updates: 1700\n",
      "Episode 33273: Saved model weights and log.\n",
      "Steps : 17050000, Average Reward: 39.24742268041237, Memory Length: 150000, Optimizer Steps: 17050000, Time Elapsed: 365.68290758132935, Target Q Updates: 1705\n",
      "Steps : 17100000, Average Reward: 37.802083333333336, Memory Length: 150000, Optimizer Steps: 17100000, Time Elapsed: 368.27574348449707, Target Q Updates: 1710\n",
      "Steps : 17150000, Average Reward: 32.70873786407767, Memory Length: 150000, Optimizer Steps: 17150000, Time Elapsed: 366.5316798686981, Target Q Updates: 1715\n",
      "Steps : 17200000, Average Reward: 35.49, Memory Length: 150000, Optimizer Steps: 17200000, Time Elapsed: 367.5085711479187, Target Q Updates: 1720\n",
      "Steps : 17250000, Average Reward: 34.45454545454545, Memory Length: 150000, Optimizer Steps: 17250000, Time Elapsed: 367.08017921447754, Target Q Updates: 1725\n",
      "Steps : 17300000, Average Reward: 37.45360824742268, Memory Length: 150000, Optimizer Steps: 17300000, Time Elapsed: 366.81693983078003, Target Q Updates: 1730\n",
      "Steps : 17350000, Average Reward: 31.540816326530614, Memory Length: 150000, Optimizer Steps: 17350000, Time Elapsed: 367.8907251358032, Target Q Updates: 1735\n",
      "Steps : 17400000, Average Reward: 30.902912621359224, Memory Length: 150000, Optimizer Steps: 17400000, Time Elapsed: 367.83386635780334, Target Q Updates: 1740\n",
      "Steps : 17450000, Average Reward: 37.2680412371134, Memory Length: 150000, Optimizer Steps: 17450000, Time Elapsed: 367.9179425239563, Target Q Updates: 1745\n",
      "Steps : 17500000, Average Reward: 37.01030927835052, Memory Length: 150000, Optimizer Steps: 17500000, Time Elapsed: 366.31248021125793, Target Q Updates: 1750\n",
      "Episode 34260: Saved model weights and log.\n",
      "Steps : 17550000, Average Reward: 28.805825242718445, Memory Length: 150000, Optimizer Steps: 17550000, Time Elapsed: 368.83633852005005, Target Q Updates: 1755\n",
      "Steps : 17600000, Average Reward: 31.35576923076923, Memory Length: 150000, Optimizer Steps: 17600000, Time Elapsed: 367.5245842933655, Target Q Updates: 1760\n",
      "Steps : 17650000, Average Reward: 37.33673469387755, Memory Length: 150000, Optimizer Steps: 17650000, Time Elapsed: 366.50636744499207, Target Q Updates: 1765\n",
      "Steps : 17700000, Average Reward: 31.663366336633665, Memory Length: 150000, Optimizer Steps: 17700000, Time Elapsed: 367.88291025161743, Target Q Updates: 1770\n",
      "Steps : 17750000, Average Reward: 32.2970297029703, Memory Length: 150000, Optimizer Steps: 17750000, Time Elapsed: 366.69683027267456, Target Q Updates: 1775\n",
      "Steps : 17800000, Average Reward: 34.56, Memory Length: 150000, Optimizer Steps: 17800000, Time Elapsed: 366.6128726005554, Target Q Updates: 1780\n",
      "Steps : 17850000, Average Reward: 29.702970297029704, Memory Length: 150000, Optimizer Steps: 17850000, Time Elapsed: 366.8866012096405, Target Q Updates: 1785\n",
      "Steps : 17900000, Average Reward: 33.26262626262626, Memory Length: 150000, Optimizer Steps: 17900000, Time Elapsed: 366.80648589134216, Target Q Updates: 1790\n",
      "Steps : 17950000, Average Reward: 39.98969072164948, Memory Length: 150000, Optimizer Steps: 17950000, Time Elapsed: 367.17617988586426, Target Q Updates: 1795\n",
      "Steps : 18000000, Average Reward: 44.634408602150536, Memory Length: 150000, Optimizer Steps: 18000000, Time Elapsed: 368.17424273490906, Target Q Updates: 1800\n",
      "Episode 35257: Saved model weights and log.\n",
      "Steps : 18050000, Average Reward: 31.08910891089109, Memory Length: 150000, Optimizer Steps: 18050000, Time Elapsed: 372.4831008911133, Target Q Updates: 1805\n",
      "Steps : 18100000, Average Reward: 37.92783505154639, Memory Length: 150000, Optimizer Steps: 18100000, Time Elapsed: 365.8650732040405, Target Q Updates: 1810\n",
      "Steps : 18150000, Average Reward: 29.34285714285714, Memory Length: 150000, Optimizer Steps: 18150000, Time Elapsed: 366.4136507511139, Target Q Updates: 1815\n",
      "Steps : 18200000, Average Reward: 37.366336633663366, Memory Length: 150000, Optimizer Steps: 18200000, Time Elapsed: 366.7829089164734, Target Q Updates: 1820\n",
      "Steps : 18250000, Average Reward: 45.829787234042556, Memory Length: 150000, Optimizer Steps: 18250000, Time Elapsed: 366.4245824813843, Target Q Updates: 1825\n",
      "Steps : 18300000, Average Reward: 32.088235294117645, Memory Length: 150000, Optimizer Steps: 18300000, Time Elapsed: 367.7678053379059, Target Q Updates: 1830\n",
      "Steps : 18350000, Average Reward: 34.07, Memory Length: 150000, Optimizer Steps: 18350000, Time Elapsed: 366.2410352230072, Target Q Updates: 1835\n",
      "Steps : 18400000, Average Reward: 43.857142857142854, Memory Length: 150000, Optimizer Steps: 18400000, Time Elapsed: 367.01411867141724, Target Q Updates: 1840\n",
      "Steps : 18450000, Average Reward: 31.724489795918366, Memory Length: 150000, Optimizer Steps: 18450000, Time Elapsed: 368.62412667274475, Target Q Updates: 1845\n",
      "Steps : 18500000, Average Reward: 37.677083333333336, Memory Length: 150000, Optimizer Steps: 18500000, Time Elapsed: 366.39855885505676, Target Q Updates: 1850\n",
      "Episode 36242: Saved model weights and log.\n",
      "Steps : 18550000, Average Reward: 37.898989898989896, Memory Length: 150000, Optimizer Steps: 18550000, Time Elapsed: 368.92585945129395, Target Q Updates: 1855\n",
      "Steps : 18600000, Average Reward: 35.1, Memory Length: 150000, Optimizer Steps: 18600000, Time Elapsed: 369.89273977279663, Target Q Updates: 1860\n",
      "Steps : 18650000, Average Reward: 36.95959595959596, Memory Length: 150000, Optimizer Steps: 18650000, Time Elapsed: 368.9288625717163, Target Q Updates: 1865\n",
      "Steps : 18700000, Average Reward: 31.115384615384617, Memory Length: 150000, Optimizer Steps: 18700000, Time Elapsed: 368.87481331825256, Target Q Updates: 1870\n",
      "Steps : 18750000, Average Reward: 35.88775510204081, Memory Length: 150000, Optimizer Steps: 18750000, Time Elapsed: 367.437504529953, Target Q Updates: 1875\n",
      "Steps : 18800000, Average Reward: 34.97029702970297, Memory Length: 150000, Optimizer Steps: 18800000, Time Elapsed: 367.5481572151184, Target Q Updates: 1880\n",
      "Steps : 18850000, Average Reward: 33.679611650485434, Memory Length: 150000, Optimizer Steps: 18850000, Time Elapsed: 366.62776732444763, Target Q Updates: 1885\n",
      "Steps : 18900000, Average Reward: 37.111111111111114, Memory Length: 150000, Optimizer Steps: 18900000, Time Elapsed: 367.3123939037323, Target Q Updates: 1890\n",
      "Steps : 18950000, Average Reward: 33.81372549019608, Memory Length: 150000, Optimizer Steps: 18950000, Time Elapsed: 366.6367754936218, Target Q Updates: 1895\n",
      "Steps : 19000000, Average Reward: 36.310679611650485, Memory Length: 150000, Optimizer Steps: 19000000, Time Elapsed: 366.5867302417755, Target Q Updates: 1900\n",
      "Episode 37250: Saved model weights and log.\n",
      "Steps : 19050000, Average Reward: 37.81730769230769, Memory Length: 150000, Optimizer Steps: 19050000, Time Elapsed: 367.29461789131165, Target Q Updates: 1905\n",
      "Steps : 19100000, Average Reward: 34.44117647058823, Memory Length: 150000, Optimizer Steps: 19100000, Time Elapsed: 368.3983795642853, Target Q Updates: 1910\n",
      "Steps : 19150000, Average Reward: 37.14705882352941, Memory Length: 150000, Optimizer Steps: 19150000, Time Elapsed: 366.4416000843048, Target Q Updates: 1915\n",
      "Steps : 19200000, Average Reward: 37.15533980582524, Memory Length: 150000, Optimizer Steps: 19200000, Time Elapsed: 366.7986888885498, Target Q Updates: 1920\n",
      "Steps : 19250000, Average Reward: 34.0, Memory Length: 150000, Optimizer Steps: 19250000, Time Elapsed: 367.63668608665466, Target Q Updates: 1925\n",
      "Steps : 19300000, Average Reward: 35.97959183673469, Memory Length: 150000, Optimizer Steps: 19300000, Time Elapsed: 367.86589765548706, Target Q Updates: 1930\n",
      "Steps : 19350000, Average Reward: 30.933333333333334, Memory Length: 150000, Optimizer Steps: 19350000, Time Elapsed: 368.3943758010864, Target Q Updates: 1935\n",
      "Steps : 19400000, Average Reward: 31.142857142857142, Memory Length: 150000, Optimizer Steps: 19400000, Time Elapsed: 366.49164295196533, Target Q Updates: 1940\n",
      "Steps : 19450000, Average Reward: 35.49514563106796, Memory Length: 150000, Optimizer Steps: 19450000, Time Elapsed: 367.00511145591736, Target Q Updates: 1945\n",
      "Steps : 19500000, Average Reward: 39.11, Memory Length: 150000, Optimizer Steps: 19500000, Time Elapsed: 377.1743686199188, Target Q Updates: 1950\n",
      "Episode 38277: Saved model weights and log.\n",
      "Steps : 19550000, Average Reward: 37.31683168316832, Memory Length: 150000, Optimizer Steps: 19550000, Time Elapsed: 371.03478169441223, Target Q Updates: 1955\n",
      "Steps : 19600000, Average Reward: 32.75471698113208, Memory Length: 150000, Optimizer Steps: 19600000, Time Elapsed: 378.25738430023193, Target Q Updates: 1960\n",
      "Steps : 19650000, Average Reward: 30.38095238095238, Memory Length: 150000, Optimizer Steps: 19650000, Time Elapsed: 381.68047070503235, Target Q Updates: 1965\n",
      "Steps : 19700000, Average Reward: 28.160377358490567, Memory Length: 150000, Optimizer Steps: 19700000, Time Elapsed: 378.91095066070557, Target Q Updates: 1970\n",
      "Steps : 19750000, Average Reward: 31.29245283018868, Memory Length: 150000, Optimizer Steps: 19750000, Time Elapsed: 376.85407638549805, Target Q Updates: 1975\n",
      "Steps : 19800000, Average Reward: 42.21, Memory Length: 150000, Optimizer Steps: 19800000, Time Elapsed: 370.31512475013733, Target Q Updates: 1980\n",
      "Steps : 19850000, Average Reward: 36.878787878787875, Memory Length: 150000, Optimizer Steps: 19850000, Time Elapsed: 374.0855567455292, Target Q Updates: 1985\n",
      "Steps : 19900000, Average Reward: 41.39795918367347, Memory Length: 150000, Optimizer Steps: 19900000, Time Elapsed: 375.1054856777191, Target Q Updates: 1990\n",
      "Steps : 19950000, Average Reward: 30.31132075471698, Memory Length: 150000, Optimizer Steps: 19950000, Time Elapsed: 373.7071166038513, Target Q Updates: 1995\n",
      "Steps : 20000000, Average Reward: 36.1078431372549, Memory Length: 150000, Optimizer Steps: 20000000, Time Elapsed: 371.79046726226807, Target Q Updates: 2000\n",
      "Episode 39306: Saved model weights and log.\n"
     ]
    }
   ],
   "source": [
    "standard_train(agent, env, **training_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b38b5277-0995-43bd-be2e-7f9fd012cb34",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../models/breakout/breakout ddqn final.pth'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "194cbd04-1337-4d87-8ec6-c9dfb339171e",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757526f7-5aed-40f8-b2e4-a1dd75018edd",
   "metadata": {},
   "source": [
    "## Watch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c88e1a7b-c2f8-4138-b4bf-e2341d278b4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward: 165.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "render_agent(agent, env, path, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e24560-1941-4117-91f2-e3ff7858d363",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7281e9c6-90f8-4c05-b95b-eca73c15c13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "867fe171-e368-49b3-87c6-9bc3ba474bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_info = pd.read_csv('../atari/csvs/breakout_ddqn_training_info.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c95474e4-894f-4f8d-a232-644373b721de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1bd2b0f5760>]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(training_info['training_step'], training_info[' return'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
